version: '3.8'

# Set environment variables for Airflow and Postgres
x-airflow-common:
  &airflow-common
  image: apache/airflow:2.8.1-python3.11 # Use a specific version
  # Set User ID to prevent permission issues with mounted volumes
  user: "${AIRFLOW_UID:-50000}:0" 
  restart: always
  environment:
    # Airflow Metadata DB connection (using our Postgres service 'db')
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-shopzada123}@db_dwh:5432/shopzada_dwh
    # Important: Set a key for webserver security
    AIRFLOW__WEBSERVER__SECRET_KEY: this-is-a-temporary-key-for-shopzada
    # Specify the modules Airflow needs (for DWH connection and file handling)
    _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-postgres pandas pyarrow"
    # Create the default admin user automatically for quick startup
    _AIRFLOW_WWW_USER_USERNAME: airflow
    _AIRFLOW_WWW_USER_PASSWORD: airflow
  volumes:
    # 1. DAGs: Map local 'workflows' folder to container's DAGs directory
    - ./workflows:/opt/airflow/dags
    # 2. Logs: Persistent logs for debugging
    - ./logs:/opt/airflow/logs
    # 3. Plugins/Config: For future customization
    - ./plugins:/opt/airflow/plugins
    - ./config:/opt/airflow/config
  # ALL Airflow services only wait for the Database to be healthy
  depends_on:
    db_dwh:
      condition: service_healthy

services:
  # 1. PostgreSQL Database (DWH) - Our actual Data Warehouse DB
  db_dwh:
    image: postgres:16-alpine
    container_name: shopzada-db-dwh
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-shopzada123} 
      POSTGRES_DB: shopzada_dwh
    ports:
      - "5432:5432"   # DWH DB port (you can change if you need to access both on the same machine)
    volumes:
      - postgres_dwh_data:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  # 2. PostgreSQL Database (Staging) - Our Staging DB
  db_staging:
    image: postgres:16-alpine
    container_name: shopzada-db-staging
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-shopzada123} 
      POSTGRES_DB: shopzada_staging
    ports:
      - "5433:5432"  # Staging DB port (mapped to 5433 on host to avoid conflict)
    volumes:
      - postgres_staging_data:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  # 3. ETL Service - Runs the custom Python scripts using our custom image
  etl-service:
    build:
      context: ../ 
      dockerfile: infra/Dockerfile
    container_name: shopzada-etl
    # Mount the project files so Python can access them
    volumes:
      - ../:/usr/src/app/
    depends_on:
      db_dwh:
        condition: service_healthy 
    environment:
      # Connection details for Python scripts to access the DWH
      DB_HOST: db_dwh
      DB_USER: ${POSTGRES_USER:-postgres}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-shopzada123}
      AIRFLOW_UID: ${AIRFLOW_UID:-50000} # Inherit UID from the shared config

  # 4. Airflow Initialization Service - Runs DB migrations once
  airflow-init:
    <<: *airflow-common
    container_name: shopzada-airflow-init
    # FIXED COMMAND: Using 'airflow' command directly
    command: ["bash", "-c", "airflow db migrate && airflow users create --username airflow --password airflow --firstname Airflow --lastname Admin --role Admin --email admin@example.com"]
    # The init service should run and exit, so we set this to 'no'
    restart: "no"

  # 5. Airflow Webserver - Provides the UI
  airflow-webserver:
    <<: *airflow-common
    container_name: shopzada-airflow-webserver
    command: ["bash", "-c", "airflow webserver"] # Now starts the webserver
    ports:
      - "8080:8080" # Exposed port for the Airflow UI
    healthcheck:
      test: ["CMD", "curl", "--silent", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 30s
      retries: 3
    depends_on:
      db_dwh:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

 # 6. BI Dashboard Service (Placeholder) - Commented out
  # dashboard-service:
  #   image: placeholder/bi-tool:latest 
  #   container_name: shopzada-dashboard
  #   ports:
  #     - "3000:3000" 
  #   depends_on:
  #     db:
  #       condition: service_healthy
  
  # 6. Airflow Scheduler - Monitors tasks and triggers execution
  airflow-scheduler:
    <<: *airflow-common
    container_name: shopzada-airflow-scheduler
    command: ["bash", "-c", "airflow scheduler"] # Now starts the scheduler
    depends_on:
      db_dwh:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully

volumes:
  postgres_dwh_data:
    # Volume to persist PostgreSQL data for DWH
  postgres_staging_data:
    # Volume to persist PostgreSQL data for Staging
  logs:
    # Volume to persist Airflow logs
  plugins:
    # Volume for Airflow plugins
  config:
    # Volume for Airflow configuration files
